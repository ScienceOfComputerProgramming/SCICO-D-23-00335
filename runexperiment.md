# Model-based Player Experience (PX) Testing

This artifact provides the experiments that were carried out in this paper:

   * Saba Gholizadeh Ansari, I. S. W. B. Prasetya, Davide Prandi, Fitsum Meshesha Kifetew, Frank Dignum, Mehdi Dastani, Gabriele Keller, _Model-based Player Experience Testing with Emotion Pattern Verification_, in the 26th International Conference on Fundamental Approaches to Software Engineering (FASE), 2023.

The experiments aim to demonstrate and study automated player experience (PC) testing on computer games through a model-based approach.

_Experiment subject:_ a game called Lab Recruits, see: https://github.com/iv4xr-project/labrecruits.

## Artifact content

Unzipping the artifact should create this directory structure:

```
px-mbt
 |-eplaytesting-pipeline
 |  |- ...
 |  |- Data-Wave the flag level
 |-SBTtest
 |-MCtest
 |-Combinedtest
 |-traces
 |-fixedtraces
 |-iv4xrDemo
 |  |-gym
 |-mavenrepo
 |-otherNeededSoftware
```

   * `eplaytesting-pipeline` This contains the Java-project implementing the method  in the paper Ansari et al. It also contains scripts to re-run the experiments in the paper.

   * `eplaytesting-pipeline\Data-Wave the flag level` contains test suites and emotion traces that were **generated by the original experiments** in the paper. More precisely it contains:
      * `Level- level EFSM model` : the base game-level used in the experiment, along with its EFSM model.
      * `10 Selected Mutatnts of the level`: 10 mutants of the base-level.
      * `Level generated Test Suites`: the test suites used in the paper, generated by search-based-testing (SBT) and model-checking-based (MC) generators.
      * `Sec5.2-Experiment Result- Emotion traces for the original level`: emotion traces produced by running the above test suites.
      * `Sec5.3- Experiment result-Emotion Trace data for 10 mutants`: emotion traces produced by the mutants.
   * `SBTtest`, `MCtest`, `Combinedtest`: if you want to generate the test suites anew (rather than using those pre-provided above), they will be placed in these folders.
   * `traces`: if you want to re-run the test suites, the produced emotion traces will be placed in this folder.
   * `iv4xrDemo`: containing the System under Test executable.

## Hardware Requirment


Experiments were run on a machine with the following specifiacrtion:
  * 8th generation Intel Core i7 processor
  * 32 GB RAM
  * 1.8 GHz CPU
We recommend to use at least:
  * Intel Core i5
  * 8 GB Ram.

## Steps to take in the FASE 2022 VM

This assumes you have the VM already installed. Else, you can get it from here: https://zenodo.org/record/7446277#.Y7Q2Z-zMJTY

1. unzip this artifact in your computer, and share its root folder `px-mbt` to the VM. Mount it to your home directory in the VM. For example, if your home is `/home/fase2023` then mount the artifact-root folder to the location `/home/fase2023/px-mbt` in the VM.

2. Create an `.m2` folder for Maven, and set it to use a folder in the artifact as its repository; this folder is pre-populated with the needed Java jars for the experiments. So, go to the folder `px-mbt` and do:

```
> mkdir ~/.m2
> cp mavenrepo/settings.xml ~/.m2/
```

Note: if you are prevented to go into `px-mbt` due to permission denied, try this: `sudo adduser $USER vboxsf`. Reset the VM and then try again.

3. Install maven. The needed package is provided in the artifact. Go to the folder `px-mbt` and do:

```
> cd ./otherNeededSoftware/maven
> sudo dpkg -i *.deb
```

Note: if that complains, try to do `sudo apt --fix-broken install`; you  may need to turn on Internet for this.


4. We also need some Python packages. Go to the folder `px-mbt` and do:

```
> cd ./otherNeededSoftware/python
> pip3 install *.whl
```

4. That's it, the VM is now ready to run the experiments. To check if the setup work, Go to the folder `px-mbt` then:

```
> cd ./eplaytesting-pipeline
> mvn compile
```

If this complains, you can turn Internet on, then do `mvn compile` again. Then you can turn off Internet again.

## Building the java classes

They should be pre-built and included in the zip. But just in case you need to rebuild, go to `eplaytesting-pipeline` then just do `mvn clean` followed by `mvn compile`.

## The System under Test (experiment subject): the game Lab Recruits

It is a game called Lab Recruits, see: https://github.com/iv4xr-project/labrecruits. Windows, Mac, and Linux executables are included in this artifact. The game requires a level (a 'game world'). The level used by the paper is in the folder `eplaytesting-pipeline\Data-Wave the flag level\Level- level EFSM model`.

If you want to try to play it, just run the game, and load the level-definition file `LabRecruits_level.csv`in that folder. Playing with graphics is only possible on Windows and Mac.

### Caution

The original experiments were run on Windows. The game Lab Recruits uses graphics and is not really meant to be played on Linux. We do provide executable for Linux, though without graphics. The experiments are by default set to run without graphics, but if you do want to see the graphics then you need a Windows or Mac machine.

## Workflow

Generally the workflow is as follows:

   * _Step 1:_ generate test suites from an EFSM model of a Lab Recruits level. For reference, the used model and level can be found/viewed in this folder: `eplaytesting-pipeline\Data-Wave the flag level\Level- level EFSM model`. This process is offline (does not require runs on the SUT).
   * _Step 2:_ run (selected) test cases on the SUT. This uses the iv4xr agent Framework along with the OCC Computational Model of Emotion. Running test cases produce emotion traces, showing the intensity of various emotions through the runs, as calculated by the OCC model.
   * _Step 2b:_ apply trace-fixing.
   * _Step 3:_ apply Player Experience (PX) analyses on the resulting emotion traces (from step-2). Two analyses are provided in this artifact: (a) producing emotion heatmaps, as in Figure 4 and 5 in the paper, and (b) checking if the expected presence or absence of emotion patterns, as in Table 3 in the paper.

   You can do these analyses **on prepared traces (from the original experiments)** that are also packaged in this artifact (in the folder `eplaytesting-pipeline\Data-Wave the flag level\Sec5.2-Experiment Result- Emotion traces for the original level`), **or on traces you generate yourself** from step-2 above.

For convenience experiments are run using Java JUnit runner. That is, we only use Junit to borrow its runner, and not for doing unit-test.

How to do the Mutation Testing analysis (Table 4 in the paper) will be explained separately as the setup is a bit more involved.


## Step-1: generating test suites

As said, this stage is offline (does not require runs on the SUT). Test suites are generated from a supplied EFSM model of the SUT.

Pre-generated test suites are provided in the folder `eplaytesting-pipeline\Data-Wave the flag level\Level generated Test Suites`. These are the test suites originally used in the paper.

If you want to try to generate the test suites yourself, go to `eplaytesting-pipeline`. From there, run these:

```
> mvn test -Dtest=eu.iv4xr.ux.pxtestingPipeline.SBtest_Generation
> mvn test -Dtest=eu.iv4xr.ux.pxtestingPipeline.MCtest_Generation
```

The first use a search-based testing algorithm (SBT-MOSA) to generate a test suite. The second uses an LTL model checker to do the same. The resulting test suites will be placed in `SBTTest` and `MCtest` respectively. The Model Checker is deterministic (always produce the same test suite). The SBT generator is stochastic, so it may produce a different test suite every time. In the original experiment we ran it 10x; we include one of these in this artifact.

Note: the above are NOT JUnit tests, we just use its runner for convenience.

Each test case in the suites consists of three files:

  * xxxtest_k.txt : a text-readable representation of the test case (as a sequence of steps).
  * xxxtest_k.ser : a binary representation of the test case. This will be loaded when we want to run the test-case.
  * xxxtest_k.dot : visualization in the dot-format. Less important.

## Step-2: selecting, running the test cases and generating the emotion traces

To do player experience analyses we first need to run test cases on the SUT. The OCC Computational Model of Emotion is hooked into the tool that runs the test cases, so that running them generate emotion trace-files, recording the intensity of every emotion types through out the game plays as calculated by the OCC model. The player experience analyses will be later done offline on the generated traces.

To run test cases and generate emotion traces the steps:

   1. Well, you need test cases. **Select** some or all the test cases from `SBTTest` or/and `MCtest` and **copy** those to the folder `Combinedtest`. **Keep in mind that running them all will take quite some time** (hours!); you can perhaps start with just selecting two or three test cases.

   You can run a subset of STB-suite and a subset of MC-suite separately (we did that in the original experiment) if you wonder if they would give different results, or just make a mixed selection.

   Else, if you want to use the original subset used in the paper, they can be found in the folder `eplaytesting-pipeline\Data-Wave the flag level\Level generated Test Suites`.  Copy them to the folder `Combinedtest`. The original experiments ran the SBT and MC suites separately.

   2. Copy the target game level:

   ```
   > cp "eplaytesting-pipeline\Data-Wave the flag level\Level- level EFSM model"  Combinedtest\model
   ```

   3. Run the tests you selected above. Go to `eplaytesting-pipeline`, then do:

   ```
   > mvn test -Dtest=eu.iv4xr.ux.pxtestingPipeline.RunOCC
   ```

   The test cases will be run on the SUT without graphics. If you want to see the graphics, turn on the flag `withGraphics` in the class `RunOCC` (the one that you run above) to `true`.

   The resulting trace-files are placed in `eplaytesting-pipeline`. They are named `data_goalQuestCompleted_xxx.csv`



Note-1: the above is NOT a JUnit test, we just use its runner for convenience.  

Note-2: the interface to the SUT does not allow a synchronous control over the SUT. Because of this the test agent might get stuck e.g. around a sticking corner. If you suspect this might be an issue, you can run a testcase-run fixer. It will check the trace-files if they look strange. The test-cases whose traces look strange will be re-run. To run the fixer:

```
mvn test -Dtest=eu.iv4xr.ux.pxtestingPipeline.TestcasesExecRepair
```

The resulting fixed trace can be found in the folder `fixedtraces`.

## Step-3: run PX analyses

The analyses work on the emotion traces produced in Step-2 above. They should be in the folder `traces`. Alternatively, you can copy emotion traces from the original experiment from `eplaytesting-pipeline\Data-Wave the flag level` to the folder `traces`.

### Emotion heatmaps

To produce emotion heatmaps from the traces (as in Figure 4 and 5 in the paper), Go to `eplaytesting-pipeline`, then do:

```
> python3 ./mkHeatmaps.py
```

This will produce 6x heatmaps, one for each emotion type. The  heatmap of emotion type E, e.g. fear, aggregates the data from **all traces** in the folder `traces`, showing the maximum intensity of fear at every 'square' area in the game level, over all runs in the test suite (that were used to produce the traces).

### Checking PX properties

Section 4.3 in the paper discusses the use of patterns such as "H;nD;H" to capture Player Experience (PX) requirements (see the paper for the explanation on the meaning of such a pattern). To check the PX requirements listed in Table 3, go to `eplaytesting-pipeline`, then do:

```
> mvn test -Dtest=eu.iv4xr.ux.pxtestingPipeline.CheckPXProperties
```

This will check those PX requirements on the emotion traces in the folder `traces`. The results are printed to the console e.g.

```
** nD;S: 2 (VALID)
** nF;S: 1 (SAT)
** J;nS: 0 (UNSAT)
** J;D: 0 (UNSAT)
** J;F;S: 1 (SAT)
** D;H;P: 0 (UNSAT)
** D;H;S: 0 (UNSAT)
** D;H;nD;S: 0 (UNSAT)
** F;D;H;F;J: 0 (UNSAT)
** H;F;D;D;D;H;F;J: 0 (UNSAT)
** F;D;D;H;F;P: 0 (UNSAT)
** #test-cases: 2
** #emotion patterns investigated: 11
** #patterns witnessed: 3
** #patterns not appearing: 8
```

When the verdict of a property is **VALID** it means that the property occurs in (is satisfied by) all traces. **SAT** means that it occurs in one (but not all) traces. **UNSAT** means that it does NOT occur in any trace.

## Running the mutation test

Section 5.3 of the paper discussed how well our automated PX-testing approach in detecting PX errors. This is studied by running a mutation test, and using PX properties as discussed in Section 4.3 as test oracles.

10 mutants used in the paper is provided in  the folder `eplaytesting-pipeline\10 Selected Mutatnts of the level`. Each file there is a mutated level-definition of the original level (called `LabRecruits_level.csn`) in
`eplaytesting-pipeline\Level- level EFSM model`.

To replicate the mutation test, you can re-run the Steps 2.2 - 3 above for each mutant:

  1. Copy a mutatated level `xWave-the-flagxxx.csv` to the folder `Combinedtest\model`.
  2. Redo Step 2.3
  3. Redo Step 3, Checking PX Properties. You can see in the Console output which PX properties are satisfies (they all are expected to be at least satisfied).

## That was it :smile:

## Other things you might wonder:

* How does this "OCC" model work? See:

    [_An Appraisal Transition System for Event-Driven Emotions in Agent-Based Player Experience Testing_](https://doi.org/10.1007/978-3-030-97457-2_9), Ansari, Prasetya, Dastani, Dignum, Keller. In International Workshop on Engineering Multi-Agent Systems (EMAS), 2021.

* The above OCC provides the general part of emotion 'simulation'. An SUT-specific part called 'Player Characterization' needs to be added too. See the paper above, or the FASE23 paper for explanation of the role of this Characterization. For the experiment, the used Player Characterization can be found in the class `eu.iv4xr.ux.pxtestingPipeline.PlayerOneCharacterization`.
